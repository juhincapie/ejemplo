{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TALLER 3\n",
    "\n",
    "1. Hacer lectura de las datas __capacity.xlsx__, __providers__, __hospitals_migration.parquet__\n",
    "> __NOTA__: Debe cargar en Sqlite la data de __hospitals_migration.parquet__ con el objetivo de simular la consulta sobre base de datos en motor\n",
    "2. Hacer merge entre las datas __capacity.xlsx__ y __providers__ e identificarla como __original_reps__ teniendo en cuenta:\n",
    "* Se deben eliminar los atributos __habi_codigo_habilitacion__ y __tido_codigo__\n",
    "* El atributo de relación es __codigo_habilitacion__\n",
    "* La data de prestadores no debe tener duplicidad según atributo __codigo_habilitacion__\n",
    "3. se debe limpiar la data de hospitales y de original_reps teniendo en cuenta:\n",
    "* filtrar de la data de original_reps['clpr_nombre'] de tal manera que elimine los registros iguales a 'Profesional Independiente'\n",
    "* se deben eliminar los caracteres especiales: ['Á', 'É', 'Í', 'Ó', 'Ú','À', 'È', 'Ì', 'Ò', 'Ù', '  ', '(', ')']\n",
    "\n",
    "* finalmente debe identificar atributos homólogos entre hospitales y original_reps para identificar de manera exploratoria que hospitales no se encuentran en la data de original_reps. Finalmente debe reportar en csv los los hospitales faltantes, la data de original_reps limpia y hospitales limpia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué le mejorarían?\n",
    "* una función debe hacer una sola función\n",
    "* si las datas a unificar comparten atributos, aplique un pop sobre dichas columnas para no generar duplicidad de atributos. Esto reduciría en un parametro la función y eliminaría un asignación durante la orquestación.\n",
    "* Industrializar la consulta a realizar en la función query_tables\n",
    "* Documentar cada función o documentación general del generador de productos de datos\n",
    "* crear una función para cargue de producto en los formatos que se proyecten obtener\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastparquet\n",
    "import xlrd\n",
    "import xlsxwriter\n",
    "import openpy\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conection_sqlite():\n",
    "    conn = sqlite3.connect(\":memory:\")  ## aca se indica el nombre de la db.\n",
    "    cur = conn.cursor()\n",
    "    return conn, cur\n",
    "\n",
    "def read_data_parquet(in_file):\n",
    "    data = pd.read_parquet(in_file)\n",
    "    return data\n",
    "\n",
    "def read_data_xlsx(in_file):\n",
    "    data = pd.read_excel(in_file, header=0, dtype=str)\n",
    "    return data\n",
    "\n",
    "def create_table_in_sqlite(data, conn, name_table):\n",
    "    data.to_sql(\n",
    "        name=name_table,\n",
    "        con=conn,\n",
    "        if_exists=\"replace\",\n",
    "    )\n",
    "def query_tables(conn, table_name):\n",
    "    query = 'SELECT * FROM {}'.format(table_name) # esto se puede industrializar para que en la orquestación genere la consulta que se considere pertienente\n",
    "    df_table = pd.read_sql(query, con=conn)\n",
    "    return df_table\n",
    "\n",
    "def get_merge_original_data_reps(list_tables, list_of_list_atributtes_delete, keys_merge, atributtes_merge):\n",
    "    iteration = 0\n",
    "    # ________________eliminación atributos____________________________\n",
    "    for dataframe in list_tables:\n",
    "        for atributte in list_of_list_atributtes_delete[iteration]:\n",
    "            dataframe.pop(atributte)\n",
    "        iteration += 1\n",
    "    # __________________unión de fuentes reps____________________________\n",
    "    data = pd.merge(list_tables[0], list_tables[1][atributtes_merge + [\n",
    "                    keys_merge[1]]], left_on=keys_merge[0], right_on=keys_merge[1], how='left')\n",
    "    return data\n",
    "\n",
    "\n",
    "def filter_data(data, list_atributtes_filter, list_conditionals_filter):\n",
    "    iterator = 0\n",
    "    for atributte in list_atributtes_filter:\n",
    "        for conditional in list_conditionals_filter[iterator]:\n",
    "            data = data[data[atributte] != conditional]\n",
    "        iterator += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def clean_characters_special(data, specials_characters, correct_specials_characters, list_atributtes_correct):\n",
    "    for atributte in list_atributtes_correct:\n",
    "        iterator_character = 0\n",
    "        for character in specials_characters:\n",
    "            if character != specials_characters[-1]:\n",
    "                data[atributte] = data[atributte].str.upper().apply(lambda x: x.replace(\n",
    "                    character, correct_specials_characters[iterator_character]).strip())\n",
    "                iterator_character += 1\n",
    "            else:\n",
    "                data[atributte] = data[atributte].str.upper().apply(lambda x: x.replace(\n",
    "                    character, correct_specials_characters[iterator_character]).strip())\n",
    "                iterator_character = 0\n",
    "    return data\n",
    "\n",
    "\n",
    "def comparative_original_reps_hospitales(data_reps_original, data_hospitals, column_reps, column_hospitals):\n",
    "    hospitals = data_hospitals.copy()\n",
    "    reps_original = data_reps_original.copy()[[column_reps]]\n",
    "    reps_original['data_contain'] = reps_original[column_reps].apply(\n",
    "        lambda x: x.split(' ', maxsplit=len(x.split(' '))-2)[-1])\n",
    "    reps_original_list_data_contain = list(\n",
    "        reps_original['data_contain'].drop_duplicates())\n",
    "    iterator = 0\n",
    "\n",
    "    for data_contain in reps_original_list_data_contain:\n",
    "        try:\n",
    "            if iterator != 0:\n",
    "                hospitals['bool'] = np.where(hospitals[column_hospitals].str.contains(\n",
    "                    data_contain) == True, True, hospitals['bool'])\n",
    "\n",
    "            else:\n",
    "                hospitals['bool'] = np.where(\n",
    "                    hospitals[column_hospitals].str.contains(data_contain) == True, True, False)\n",
    "            iterator += 1\n",
    "        except:\n",
    "            print(data_contain)\n",
    "    hospitals_missing_in_original_reps = hospitals[hospitals['bool'] == False]\n",
    "    return hospitals_missing_in_original_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _______________________configuración 'entradas y salidas'_____________________\n",
    "conn, cur= conection_sqlite()\n",
    "provider_input = 'data_lake/landing_prestadores/providers.xlsx'\n",
    "capacity_input = 'data_lake/landing_capacidad/capacity.xlsx'\n",
    "hospital_input = 'data_lake/landing_hospitals/hospitals_migration.parquet'\n",
    "#________________________read data_____________________________________________\n",
    "provider = read_data_xlsx(provider_input)\n",
    "capacity = read_data_xlsx(capacity_input)\n",
    "hospitals_original_data = read_data_parquet(hospital_input) # Se carga esta data a SQLITE para simular procesamiento desde diferentes fuentes\n",
    "\n",
    "#________________________crear tabla hospitales en sqlite ______________________\n",
    "create_table_in_sqlite(hospitals_original_data, conn, 'hospitales')\n",
    "#_________________________leer la tabla hospitales del motor_____________________\n",
    "hospitals_migration = query_tables(conn, 'hospitales')\n",
    "\n",
    "#______________________Hacer merge entre las datas de capacidad y prestadores________\n",
    "keys_merge = ['codigo_habilitacion', 'codigo_habilitacion']\n",
    "list_of_list_atributtes_delete = [['habi_codigo_habilitacion'], ['tido_codigo']]\n",
    "atributtes_merge = ['razon_social', 'fax', 'gerente', 'fecha_radicacion','fecha_vencimiento', 'fecha_cierre', 'telefono_adicional', 'email_adicional', 'rep_legal']\n",
    "original_reps = get_merge_original_data_reps([capacity, provider], list_of_list_atributtes_delete, keys_merge, atributtes_merge)\n",
    "\n",
    "# __________________________________limpieza_____________________________________\n",
    "# ----------> Filtros\n",
    "list_atributtes_filter = ['clpr_nombre']\n",
    "list_conditionals_filter = [['Profesional Independiente']]\n",
    "original_reps_clean = filter_data(\n",
    "    original_reps, list_atributtes_filter, list_conditionals_filter)\n",
    "# --------> corrección de caracteres especiales y mayusculas\n",
    "specials_characters = ['Á', 'É', 'Í', 'Ó', 'Ú','À', 'È', 'Ì', 'Ò', 'Ù', '  ', '(', ')']\n",
    "correct_specials_characters = [\n",
    "    'A', 'E', 'I', 'O', 'U', 'A', 'E', 'I', 'O', 'U', ' ', '', '']\n",
    "hospitals_list_atributtes_correct = ['hospital']\n",
    "original_reps_list_atributtes_correct = ['sede_nombre']\n",
    "hospitals_migration_clean = clean_characters_special(\n",
    "    hospitals_migration, specials_characters, correct_specials_characters, hospitals_list_atributtes_correct)\n",
    "original_reps_clean = clean_characters_special(\n",
    "    original_reps_clean, specials_characters, correct_specials_characters, original_reps_list_atributtes_correct)\n",
    "# _______________________Hospitales que no se encuentran en reps_original pero si en hospitals_migration_________\n",
    "hospitals_missing_in_original_reps = comparative_original_reps_hospitales(\n",
    "    original_reps_clean, hospitals_migration_clean, 'sede_nombre', 'hospital')\n",
    "hospitals_missing_in_original_reps.to_csv(\n",
    "    'data_lake/reports/hospitals_missing_in_original_reps.csv')\n",
    "original_reps_clean.to_csv('data_lake/reports/original_reps_clean.csv')\n",
    "hospitals_migration_clean.to_csv(\n",
    "    'data_lake/reports/hospitals_migration_clean.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96724d6b9637267567451acd5db668b6a362e9d39d8edb46a4de17ef5590deaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
